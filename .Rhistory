color = factor(superregion2,levels=c("East & Southeast Asia",
"Former Soviet Union",
"High-Income Countries",
"Latin America & Caribbean",
"Middle East & North Africa",
"South Asia",
"Sub-Saharan Africa")))) +
geom_point(size = 2, alpha = 0.5)+
geom_smooth(se = F, alpha=0.3) +
scale_color_manual(values = c("#67001f", "#d6604d",
"#fddbc7", "#d1e5f0", "#92c5de",
"#2166ac", "#053061"))+
guides(color=guide_legend(title=NULL))+
theme_minimal() +
labs(x = "Year", y = "Grams") +
scale_x_continuous(breaks = seq(1990, 2018, 5), guide = guide_axis(angle = 0))+
theme(axis.title.y = element_text(vjust = 2),
legend.direction = "horizontal",
legend.justification = 0.05,
legend.position = "top",
legend.text = element_text(size = 6, color = "gray10"),
panel.grid.minor.y =element_blank()) +
ggtitle("Superregional Refined Grains Average Intake") +
labs(caption = "*For all ages, gender, rural/urban, edu background.")
# get the data in
non_starchy_vegs<-read.csv("data/Country-level-estimates/v02_cnty.csv")
# get the data in
non_starchy_vegs<-read.csv("data/Country-level-estimates/v02_cnty.csv")
starchy_vegs<-read.csv("data/Country-level-estimates/v04_cnty.csv")
female_nsv_18<- non_starchy_vegs %>%
# choose the year of 2018 for present
filter(year==2018,age==999,edu==999,urban==999,female==1)%>%
dplyr::select(iso3,median)%>%
arrange(desc(median))%>%
rename(non_starchy_vegs_mean=median)
female_sv_18<- starchy_vegs %>%
filter(year==2018,age==999,edu==999,urban==999,female==1)%>%
# can add a filter of gender, with two layers
dplyr::select(iso3,median)%>%
arrange(desc(median))%>%
rename(starchy_vegs_mean=median)
# add selected data from the dataset back to shape-files
combined <- world_sp@data %>%
left_join(female_nsv_18,by=c(ISO3="iso3"))%>%
left_join(female_sv_18,by=c(ISO3="iso3"))
world_sp@data <- combined
plot1 <- leaflet(world_sp,
leafletOptions(attributionControl = FALSE, minzoom=1.5)) %>%
setView(lat=10, lng=0 , zoom=2) %>%
# Base groups = Background layer
addTiles(group = "OpenStreetMap") %>%
addProviderTiles(providers$Stamen.TonerLite, group = "Toner Lite") %>%
# add country borders
addPolygons(group='Female Refined Grains Intake',
stroke = TRUE, smoothFactor = 0.5,
weight=1, color='#333333', opacity=1,
# add first layer:
fillColor = ~colorQuantile("Blues", median.x)(median.x),
fillOpacity = 1,
# add label
label = ~stringr::str_c(NAME, ' ',
formatC(median.x, big.mark = ',', format='d')),
labelOptions = labelOptions(direction = 'auto'))%>%
# add the second layer:
addPolygons(group='Male Refined Grains Intake',
stroke = TRUE, smoothFactor = 0.5,
weight=1, color='#333333', opacity=1,
fillColor = ~colorQuantile("Reds",median.y)(median.y), fillOpacity = 1,
# add label
label = ~stringr::str_c(NAME, ' ',
formatC(median.x, big.mark = ',', format='d')),
labelOptions = labelOptions(direction = 'auto'))%>%
# add layer control
addLayersControl(
baseGroups = c("OpenStreetMap", "Toner Lite"), #<<
overlayGroups = c("Female Refined Grains Intake","Male Refined Grains Intake"), #<<
options = layersControlOptions(collapsed = TRUE) )
frameWidget(plot1)
plot2 <- leaflet(world_sp,
leafletOptions(attributionControl = FALSE, minzoom=1.5)) %>%
setView(lat=10, lng=0 , zoom=2) %>%
# Base groups = Background layer
addTiles(group = "OpenStreetMap") %>%
addProviderTiles(providers$Stamen.TonerLite, group = "Toner Lite") %>%
# add country borders
addPolygons(group='Non-strachy Vegetables',
stroke = TRUE, smoothFactor = 0.5,
weight=1, color='#333333', opacity=1,
# add first layer:
fillColor = ~colorQuantile("Blues", non_starchy_vegs_mean)(non_starchy_vegs_mean),
fillOpacity = 1,
# add label
label = ~stringr::str_c(NAME, ' ',
formatC(non_starchy_vegs_mean, big.mark = ',', format='d')),
labelOptions = labelOptions(direction = 'auto'))%>%
# add the second layer:
addPolygons(group='Strachy Vegetables',
stroke = TRUE, smoothFactor = 0.5,
weight=1, color='#333333', opacity=1,
fillColor = ~colorQuantile("Reds",starchy_vegs_mean)(starchy_vegs_mean), fillOpacity = 1,
# add label
label = ~stringr::str_c(NAME, ' ',
formatC(starchy_vegs_mean, big.mark = ',', format='d')),
labelOptions = labelOptions(direction = 'auto'))%>%
# add layer control
addLayersControl(
baseGroups = c("OpenStreetMap", "Toner Lite"), #<<
overlayGroups = c("Non-strachy Vegetables","Strachy Vegetables"), #<<
options = layersControlOptions(collapsed = TRUE) )
frameWidget(plot2)
# Vegetables bar chart by continent
df01 <- non_starchy_vegs%>%
filter(age==999,female==999,urban==999,edu==999)%>%
dplyr::select(superregion2,iso3,year,median)%>%
group_by(superregion2,year)%>%
mutate(regional_avg=mean(median))%>%
select(superregion2,year,regional_avg)%>%
mutate(superregion2 = recode(superregion2, Asia = "East & Southeast Asia",
FSU = "Former Soviet Union",
HIC =  "High-Income Countries",
LAC = "Latin America & Caribbean",
MENA= "Middle East & North Africa",
SAARC ="South Asia",
SSA = "Sub-Saharan Africa"
))
df01%>%
ggplot(aes(x = year, y = regional_avg,
color = factor(superregion2,levels=c("East & Southeast Asia",
"Former Soviet Union",
"High-Income Countries",
"Latin America & Caribbean",
"Middle East & North Africa",
"South Asia",
"Sub-Saharan Africa")))) +
geom_point(size = 2, alpha = 0.5)+
geom_smooth(se = F, alpha=0.3) +
scale_color_manual(values = c("#67001f", "#d6604d",
"#fddbc7", "#d1e5f0", "#92c5de",
"#2166ac", "#053061"))+
guides(color=guide_legend(title=NULL))+
theme_minimal() +
labs(x = "Year", y = "Grams") +
scale_x_continuous(breaks = seq(1990, 2018, 5), guide = guide_axis(angle = 0))+
theme(axis.title.y = element_text(vjust = 2),
legend.direction = "horizontal",
legend.justification = 0.05,
legend.position = "top",
legend.text = element_text(size = 8, color = "gray10"),
panel.grid.minor.y =element_blank()) +
ggtitle("Superregional Non Starchy Vegetables Average Intake") +
labs(caption = "*For all ages, gender, rural/urban, edu background.")
# Load packages.
packages <- c("readr","ggplot2","tidyverse","dplyr","data.table","fs","plotly",
'glue','maps','ggpubr','ggmap','ggthemes','leaflet','leaflet.extras',
'tidytext','textdata','DT','RColorBrewer','widgetframe','rgdal')
packages <- lapply(packages, FUN = function(x) {
if(!require(x, character.only = TRUE)) {
install.packages(x)
library(x, character.only = TRUE)
}
}
)
plot1 <- leaflet(world_sp,
leafletOptions(attributionControl = FALSE, minzoom=1.5)) %>%
setView(lat=10, lng=0 , zoom=2) %>%
# Base groups = Background layer
addTiles(group = "OpenStreetMap") %>%
addProviderTiles(providers$Stamen.TonerLite, group = "Toner Lite") %>%
# add country borders
addPolygons(group='Female Refined Grains Intake',
stroke = TRUE, smoothFactor = 0.5,
weight=1, color='#333333', opacity=1,
# add first layer:
fillColor = ~colorQuantile("Blues", median.x)(median.x),
fillOpacity = 1,
# add label
label = ~stringr::str_c(NAME, ' ',
formatC(median.x, big.mark = ',', format='d')),
labelOptions = labelOptions(direction = 'auto'))%>%
# add the second layer:
addPolygons(group='Male Refined Grains Intake',
stroke = TRUE, smoothFactor = 0.5,
weight=1, color='#333333', opacity=1,
fillColor = ~colorQuantile("Reds",median.y)(median.y), fillOpacity = 1,
# add label
label = ~stringr::str_c(NAME, ' ',
formatC(median.x, big.mark = ',', format='d')),
labelOptions = labelOptions(direction = 'auto'))%>%
# add layer control
addLayersControl(
baseGroups = c("OpenStreetMap", "Toner Lite"), #<<
overlayGroups = c("Female Refined Grains Intake","Male Refined Grains Intake"), #<<
options = layersControlOptions(collapsed = TRUE) )
#frameWidget(plot1)
widgetframe::frameWidget(plot1, width = "100%")
library(tidyverse)
library(DT)
library(viridis)
library(hrbrthemes)
library(tm)
library(stringr)
library(SnowballC)
library(tidytext)
library(wordcloud)
library(quanteda)
KickStarter = read.csv('kickstarter_projects_2021-05.csv')
# check for the status value counts
dplyr::count(KickStarter,state,sort=TRUE)
# group data by `top_category` and calculate percentage of success
state <- KickStarter %>%
filter(state == 'failed' | state == 'successful') %>%
group_by(top_category,state)%>%
summarise(count = n()) %>%
mutate(ratio = round(count / sum(count),2))%>%
filter(state=="successful")%>%
select(top_category,ratio)%>%
arrange(desc(ratio))
state
# create bar plot
ggplot(data = state ,aes(x=reorder(top_category,ratio),y=ratio))+
geom_bar(stat="identity",alpha=0.4)+
geom_text(aes(label = ratio),colors='b')+
theme(legend.position = "none")+
labs(y="Success Ratio",x= 'Top Category',title="Top Categories Succeeding in KickStarter")+
coord_flip()+
theme_ipsum()
# calculate the achievement ratio by dividing the amount being pledged to the amount of goal
time <- KickStarter%>%
select(top_category,pledged,goal)%>%
mutate(achievement_ratio=round(pledged/goal *100,2))%>%
group_by(top_category) %>%
summarise(mean(achievement_ratio))%>%
rename(average_achievement_ratio ='mean(achievement_ratio)')%>%
arrange(desc(average_achievement_ratio))
time
# create a datatable for viz
datatable(time, rownames=FALSE, colnames=c("Top Category","Average Achievement Ratio"), caption=htmltools::tags$caption("Top Categories Succeeding in KickStarter"), options=list(autoWidth = TRUE, dom = "ft", pageLength=10), filter = list(position="top"))
# calculate the total number of successful projects by state
successful_case_by_state <- KickStarter %>%
filter(state=="successful") %>%
group_by(location_state) %>%
count(location_state) %>%
arrange(desc(n)) %>%
rename(`Total Successful Cases`= n)
successful_case_by_city <- KickStarter %>%
filter(state=="successful") %>%
select(location_state,location_town) %>%
group_by(location_town) %>%
mutate(n=n())%>%
arrange(desc(n)) %>%
rename(`Total Successful Cases` = n) %>%
distinct(location_town,.keep_all = TRUE)%>%
head(50)
datatable(successful_case_by_city)
# filter the failure one first
failure_sample = KickStarter %>% filter(state=='failed')
# create the sample
set.seed(2022)
text_sample = rbind(head(KickStarter %>%
filter(state=='successful') %>%
arrange(desc(backers_count)), 1000),
failure_sample[sample(nrow(failure_sample), 1000), ])
# remove punctuation
text_sample$clean_blurb = removePunctuation(text_sample$blurb)
# remove numbers
text_sample$clean_blurb = removeNumbers(text_sample$clean_blurb)
# remove fully-capitalized words
text_sample$clean_blurb = str_trim(gsub("\\b[A-Z]+\\b","", text_sample$clean_blurb))
# remove white space
text_sample$clean_blurb = stripWhitespace(text_sample$clean_blurb)
# lowercase all
text_sample$clean_blurb = tolower(text_sample$clean_blurb)
# remove stop-words
text_sample$clean_blurb = str_trim(removeWords(text_sample$clean_blurb, stopwords('en')))
# stemming
text_sample$clean_blurb = stemDocument(text_sample$clean_blurb)
text_sample$blurb[1]
text_sample$clean_blurb[1]
# bring the clean_blurb into a dataframe
df_source = data.frame(doc_id=text_sample$id, text = text_sample$clean_blurb, stringsAsFactors = FALSE)
# create a DataframeSource first, then convert to Corpus object (in the tm() package)
df_corpus = VCorpus(DataframeSource(df_source))
# create DTM
text_dtm = DocumentTermMatrix(df_corpus)
# review a portion of the matrix
as.matrix(text_dtm)[1:5,1:10]
# convert the matrix back to df
library(tidytext)
text_td = tidy(text_dtm)
text_td$document = as.numeric(text_td$document)
# bind TF, DF, IDF frequencies
text_td <-  text_td %>%
bind_tf_idf(term, document, count)
# merge the 2 documents
text = left_join(text_td, text_sample %>% select(id, state, clean_blurb),
by=c('document' = 'id'))
# set seed
set.seed(2022)
# create dataframe
text_successful <- text %>%
filter(state == 'successful')
# Produce word cloud on success  projects only
wordcloud(text_successful$term, text_successful$tf,
max.words = 100, colors = brewer.pal(8, "PuOr"),
scale = c(3, 0.2))
# collect top 20 words according to frequency
top_20_words <- text %>%
group_by(term) %>%
summarise(n = sum(count)) %>%
arrange(desc(n)) %>%
head(20)
# filter with the collected words
SampleTop20 <- text %>%
filter(term %in% top_20_words$term) %>%
group_by(term, state) %>%
summarise(frequency = sum(count))
# use  Flesh-Kincaid statistic
require(quanteda.textstats)
df_source2 = data.frame(doc_id=text_sample$id, text = text_sample$blurb, stringsAsFactors = FALSE)
sample_fre = textstat_readability(corpus(df_source2), measure=c('Flesch.Kincaid'))
sample_fre$document = as.numeric(sample_fre$document)
sample_fre = left_join(text_sample, sample_fre, by=c('id' = 'document'))
# boxplot
ggplot(sample_fre)+
geom_boxplot(aes(x=state, y=Flesch.Kincaid, fill=state))+
scale_fill_brewer(palette = "Set1")+
labs(x='State', y='Flesch-Kincaid Reading Ease',
title="Flesch-Kincaid Reading Ease of Blurbs")+
theme_ipsum()
# getting in Hu-Liu dictionary
pos <- read.table('positive-words.txt', as.is=T)
neg <- read.table('negative-words.txt', as.is=T)
# get the data, use the text_sample that had 2000 lines
text_sentiment <- text_sample %>%
select(backers_count,state,clean_blurb)
# function for sentiment analysis
sentiment <- function(words){
require(quanteda)
tok <- quanteda::tokens(words)
pos.count <- sum(tok[[1]]%in%pos[,1])
neg.count <- sum(tok[[1]]%in%neg[,1])
out <- (pos.count - neg.count)/(pos.count+neg.count)
return(out)
}
# add the sentiment score back
mylist <- c()
for(i in text_sentiment$blurb){
mylist<-c(mylist,sentiment(i))
}
text_sentiment$tone <- mylist
text_sentiment[is.na(text_sentiment)] <- 0
text_sentiment
# visualize the sentiment score
ggplot(text_sentiment, aes(x = tone, y = backers_count, size = backers_count,color = blues9)) +
geom_smooth(alpha = 0.3) +
geom_point(alpha = 0.3)+
ggtitle("Backers Count and Tone") +
xlab("Tone") +
ylab("Backers Count") +
scale_y_continuous(breaks = seq(0, 100000, 20000)) +
theme(plot.title = element_text(hjust = 0.5)) +
theme_ipsum()
# visualize the sentiment score
ggplot(text_sentiment, aes(x = state, y = backers_count, size = backers_count,color = state)) +
geom_smooth(alpha = 0.3) +
geom_point(alpha = 0.3)+
ggtitle("Backers Count and Tone") +
xlab("Tone") +
ylab("Backers Count") +
scale_y_continuous(breaks = seq(0, 100000, 20000)) +
theme(plot.title = element_text(hjust = 0.5)) +
theme_ipsum()
# visualize the sentiment score
ggplot(text_sentiment, aes(x = state, y = backers_count, size = backers_count,color = state)) +
geom_smooth(alpha = 0.3) +
geom_point(alpha = 0.3)+
ggtitle("Backers Count and Tone") +
xlab("Tone") +
ylab("Backers Count") +
scale_y_continuous(breaks = seq(0, 100000, 20000)) +
theme(plot.title = element_text(hjust = 0.5)) +
theme_ipsum()
# filter for positive and negative text document
text_sentiment$sentiment_cat <- ifelse(text_sentiment$tone > 0, 'positive', 'negative')
# function for sentiment analysis
sentiment <- function(words){
require(quanteda)
tok <- quanteda::tokens(words)
pos.count <- sum(tok[[1]]%in%pos[,1])
neg.count <- sum(tok[[1]]%in%neg[,1])
out <- (pos.count - neg.count)/(pos.count+neg.count)
return(out)
}
# add the sentiment score back
mylist <- c()
for(i in text_sentiment$blurb){
mylist<-c(mylist,sentiment(i))
}
text_sentiment$tone <- mylist
text_sentiment[is.na(text_sentiment)] <- 0
text_sentiment
# function for sentiment analysis
sentiment <- function(words){
require(quanteda)
tok <- quanteda::tokens(words)
pos.count <- sum(tok[[1]]%in%pos[,1])
neg.count <- sum(tok[[1]]%in%neg[,1])
out <- (pos.count - neg.count)/(pos.count+neg.count)
return(out)
}
# add the sentiment score back
mylist <- c()
for(i in text_sentiment$blurb){
mylist<-c(mylist,sentiment(i))
}
text_sentiment$tone <- mylist
text_sentiment[,'tone']=round(text_sentiment[,'tone'],3)
# function for sentiment analysis
sentiment <- function(words){
require(quanteda)
tok <- quanteda::tokens(words)
pos.count <- sum(tok[[1]]%in%pos[,1])
neg.count <- sum(tok[[1]]%in%neg[,1])
out <- (pos.count - neg.count)/(pos.count+neg.count)
return(out)
}
# add the sentiment score back
mylist <- c()
for(i in text_sentiment$blurb){
mylist<-c(mylist,sentiment(i))}
text_sentiment$tone <- mylist
text_sentiment[,'tone']=round(text_sentiment[,'tone'],3)
# function for sentiment analysis
sentiment <- function(words){
require(quanteda)
tok <- quanteda::tokens(words)
pos.count <- sum(tok[[1]]%in%pos[,1])
neg.count <- sum(tok[[1]]%in%neg[,1])
out <- (pos.count - neg.count)/(pos.count+neg.count)
return(out)
}
# add the sentiment score back
mylist <- c()
for(i in text_sentiment$blurb){
mylist<-c(mylist,sentiment(i))}
text_sentiment$tone <- mylist
#text_sentiment[,'tone']=round(text_sentiment[,'tone'],3)
text_sentiment[is.na(text_sentiment)] <- 0
text_sentiment
knitr::opts_chunk$set(echo = TRUE)
# Load packages.
packages <- c("readr","ggplot2","tidyverse","dplyr","data.table","fs","plotly",
'glue','maps','ggpubr','ggmap','ggthemes','leaflet','leaflet.extras',
'tidytext','textdata','DT','RColorBrewer','widgetframe','rgdal','grid',
'gridExtra','rio','textstem','tm','wordcloud2','qdapRegex')
packages <- lapply(packages, FUN = function(x) {
if(!require(x, character.only = TRUE)) {
install.packages(x)
library(x, character.only = TRUE)
}
}
)
# READ FILES
# selected countries
country_list = c('CHN','DEU','IND','JPN','PRK','PAK','PSE','SDN','SYR','USA')
# read the file with country code, country name, and life expectancy
life_df <- read_csv('data/life_exp.csv',show_col_types = FALSE) %>%
mutate(year = as.numeric(year))
# read the file with food intake
food_df <- read_csv('data/Country-level Nutritions.csv',show_col_types = FALSE)%>%
filter(varnum <15)%>%
rename(intake=median)
# read the file with food code, and food name
food_code <- read_csv('data/food_code.csv',show_col_types = FALSE)
# read the file with food code, and food name
country_code <- read_csv('data/country_code.csv',show_col_types = FALSE)
# left join the two files above
food_life <- food_df %>%
left_join(x=food_df, y=life_df, by = c('iso3'='Code','year'='year'))  %>%
left_join(.,y=food_code, by = c('varnum'='varnum'))
#word cloud with "nutritional" tag in twitter
n <- "data/nutritional"
files <- list.files(n, pattern="csv", full.names=TRUE) %>%
set_names()
wc_n <- files %>% map_dfr(read_csv, .id="filename")
n_text <- str_c(wc_n$data.text, collapse = "")
#clean the text
n_text <-
n_text %>%
str_remove("\\n") %>%                   # remove linebreaks
rm_twitter_url() %>%                    # Remove URLS
rm_url() %>%
str_remove_all("#\\S+") %>%             # Remove any hashtags
str_remove_all("@\\S+") %>%             # Remove any @ mentions
removeWords(stopwords("english")) %>%   # Remove common words (a, the, it etc.)
removeNumbers() %>%
stripWhitespace() %>%
removeWords(c("amp")) %>%
removeWords(stopwords("SMART")) %>%
removePunctuation()
removeNumPunct <- function(x){gsub("[^[:alpha:][:space:]]*", "", x)}
# Convert the data into a summary table
n_Corpus <-
Corpus(VectorSource(n_text)) %>%
tm_map(content_transformer(removeNumPunct)) %>%
TermDocumentMatrix() %>%
as.matrix()
n_Corpus <- sort(rowSums(n_Corpus), decreasing=TRUE)
n_Corpus <- data.frame(word = names(n_Corpus), freq=n_Corpus, row.names = NULL)
#wordcloud
set.seed(1000)
wordcloud2(n_Corpus, color = 'random-light', size = 2, minRotation = -pi/6, maxRotation = -pi/6)
